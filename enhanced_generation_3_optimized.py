#!/usr/bin/env python3
"""
Enhanced Generation 3: Optimized & Scalable Implementation
Adds performance optimization, caching, concurrent processing, resource pooling,
load balancing, and auto-scaling triggers.
"""

import asyncio
import logging
import json
import time
import uuid
import hashlib
import hmac
import threading
import weakref
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Set, Callable
from dataclasses import dataclass, asdict, field
from enum import Enum
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, deque
import heapq
import traceback

# Configure comprehensive logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)

class CacheStrategy(Enum):
    """Cache strategies for different data types."""
    LRU = "lru"
    LFU = "lfu"
    FIFO = "fifo"
    TTL = "ttl"

class LoadBalancingStrategy(Enum):
    """Load balancing strategies."""
    ROUND_ROBIN = "round_robin"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    ADAPTIVE = "adaptive"

class AutoScalingTrigger(Enum):
    """Auto-scaling trigger conditions."""
    CPU_THRESHOLD = "cpu_threshold"
    MEMORY_THRESHOLD = "memory_threshold"
    REQUEST_RATE = "request_rate"
    RESPONSE_TIME = "response_time"
    QUEUE_LENGTH = "queue_length"

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics."""
    requests_per_second: float = 0.0
    average_response_time: float = 0.0
    p95_response_time: float = 0.0
    p99_response_time: float = 0.0
    error_rate: float = 0.0
    cache_hit_rate: float = 0.0
    active_connections: int = 0
    queue_depth: int = 0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    
    def to_dict(self) -> Dict:
        return asdict(self)

class AdaptiveCache:
    """High-performance adaptive cache with multiple strategies."""
    
    def __init__(self, max_size: int = 1000, strategy: CacheStrategy = CacheStrategy.LRU):
        self.max_size = max_size
        self.strategy = strategy
        self.cache: Dict[str, Any] = {}
        self.access_count: Dict[str, int] = defaultdict(int)
        self.access_order: deque = deque()
        self.expiry_times: Dict[str, datetime] = {}
        self.lock = threading.RLock()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0
        }
        
        logger.info(f"üóÑÔ∏è Adaptive Cache initialized: strategy={strategy.value}, max_size={max_size}")
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache with strategy-aware access tracking."""
        with self.lock:
            # Check TTL expiry
            if key in self.expiry_times and datetime.now() > self.expiry_times[key]:
                self._remove(key)
                self.stats["misses"] += 1
                return None
            
            if key in self.cache:
                self._update_access(key)
                self.stats["hits"] += 1
                return self.cache[key]
            
            self.stats["misses"] += 1
            return None
    
    def put(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        """Put value in cache with automatic eviction."""
        with self.lock:
            # Set expiry if TTL provided
            if ttl_seconds:
                self.expiry_times[key] = datetime.now() + timedelta(seconds=ttl_seconds)
            
            # Remove existing entry to update
            if key in self.cache:
                self._remove(key)
            
            # Evict if at capacity
            while len(self.cache) >= self.max_size:
                self._evict_one()
            
            self.cache[key] = value
            self._update_access(key)
    
    def _update_access(self, key: str):
        """Update access tracking based on strategy."""
        if self.strategy == CacheStrategy.LRU:
            if key in self.access_order:
                self.access_order.remove(key)
            self.access_order.append(key)
        elif self.strategy == CacheStrategy.LFU:
            self.access_count[key] += 1
    
    def _evict_one(self):
        """Evict one item based on strategy."""
        if not self.cache:
            return
        
        evict_key = None
        
        if self.strategy == CacheStrategy.LRU:
            evict_key = self.access_order.popleft()
        elif self.strategy == CacheStrategy.LFU:
            evict_key = min(self.access_count.keys(), key=lambda k: self.access_count[k])
        elif self.strategy == CacheStrategy.FIFO:
            evict_key = next(iter(self.cache))
        
        if evict_key:
            self._remove(evict_key)
            self.stats["evictions"] += 1
    
    def _remove(self, key: str):
        """Remove key from all tracking structures."""
        self.cache.pop(key, None)
        self.access_count.pop(key, None)
        self.expiry_times.pop(key, None)
        if key in self.access_order:
            self.access_order.remove(key)
    
    def get_hit_rate(self) -> float:
        """Calculate cache hit rate."""
        total = self.stats["hits"] + self.stats["misses"]
        return self.stats["hits"] / total if total > 0 else 0.0
    
    def clear(self):
        """Clear all cache data."""
        with self.lock:
            self.cache.clear()
            self.access_count.clear()
            self.access_order.clear()
            self.expiry_times.clear()

class ConnectionPool:
    """High-performance connection pool with auto-scaling."""
    
    def __init__(self, create_connection: Callable, min_size: int = 5, max_size: int = 50):
        self.create_connection = create_connection
        self.min_size = min_size
        self.max_size = max_size
        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)
        self.active_connections: Set = set()
        self.lock = asyncio.Lock()
        self.stats = {
            "created": 0,
            "reused": 0,
            "closed": 0
        }
        
        logger.info(f"üèä Connection Pool initialized: min={min_size}, max={max_size}")
    
    async def initialize(self):
        """Initialize pool with minimum connections."""
        for _ in range(self.min_size):
            conn = await self.create_connection()
            await self.pool.put(conn)
            self.stats["created"] += 1
    
    async def acquire(self):
        """Acquire connection from pool."""
        try:
            # Try to get existing connection
            conn = self.pool.get_nowait()
            self.active_connections.add(conn)
            self.stats["reused"] += 1
            return conn
        except asyncio.QueueEmpty:
            # Create new connection if under limit
            async with self.lock:
                if len(self.active_connections) < self.max_size:
                    conn = await self.create_connection()
                    self.active_connections.add(conn)
                    self.stats["created"] += 1
                    return conn
            
            # Wait for available connection
            conn = await self.pool.get()
            self.active_connections.add(conn)
            self.stats["reused"] += 1
            return conn
    
    async def release(self, conn):
        """Release connection back to pool."""
        if conn in self.active_connections:
            self.active_connections.remove(conn)
            try:
                self.pool.put_nowait(conn)
            except asyncio.QueueFull:
                # Pool is full, close connection
                await self._close_connection(conn)
                self.stats["closed"] += 1
    
    async def _close_connection(self, conn):
        """Close a connection."""
        # Implement connection-specific cleanup
        pass

class AdaptiveLoadBalancer:
    """Intelligent load balancer with multiple strategies and auto-adaptation."""
    
    def __init__(self, strategy: LoadBalancingStrategy = LoadBalancingStrategy.ADAPTIVE):
        self.strategy = strategy
        self.node_metrics: Dict[str, PerformanceMetrics] = {}
        self.node_weights: Dict[str, float] = {}
        self.current_node_index = 0
        self.lock = threading.RLock()
        self.adaptation_interval = 30.0  # seconds
        self.last_adaptation = time.time()
        
        logger.info(f"‚öñÔ∏è Adaptive Load Balancer initialized: strategy={strategy.value}")
    
    def update_node_metrics(self, node_id: str, metrics: PerformanceMetrics):
        """Update performance metrics for a node."""
        with self.lock:
            self.node_metrics[node_id] = metrics
            self._update_weights()
    
    def _update_weights(self):
        """Update node weights based on performance."""
        if not self.node_metrics:
            return
        
        # Calculate weights based on inverse of response time and error rate
        for node_id, metrics in self.node_metrics.items():
            response_factor = 1.0 / max(metrics.average_response_time, 0.001)
            error_factor = 1.0 - min(metrics.error_rate, 0.99)
            cpu_factor = 1.0 - min(metrics.cpu_usage / 100.0, 0.99)
            
            self.node_weights[node_id] = response_factor * error_factor * cpu_factor
    
    def select_node(self, available_nodes: List[str]) -> Optional[str]:
        """Select optimal node based on current strategy."""
        if not available_nodes:
            return None
        
        with self.lock:
            # Auto-adapt strategy if needed
            if time.time() - self.last_adaptation > self.adaptation_interval:
                self._adapt_strategy()
                self.last_adaptation = time.time()
            
            if self.strategy == LoadBalancingStrategy.ROUND_ROBIN:
                return self._round_robin_select(available_nodes)
            elif self.strategy == LoadBalancingStrategy.LEAST_CONNECTIONS:
                return self._least_connections_select(available_nodes)
            elif self.strategy == LoadBalancingStrategy.WEIGHTED_ROUND_ROBIN:
                return self._weighted_select(available_nodes)
            elif self.strategy == LoadBalancingStrategy.ADAPTIVE:
                return self._adaptive_select(available_nodes)
            
            return available_nodes[0]  # Fallback
    
    def _round_robin_select(self, nodes: List[str]) -> str:
        """Simple round-robin selection."""
        selected = nodes[self.current_node_index % len(nodes)]
        self.current_node_index += 1
        return selected
    
    def _least_connections_select(self, nodes: List[str]) -> str:
        """Select node with least active connections."""
        min_connections = float('inf')
        selected_node = nodes[0]
        
        for node in nodes:
            metrics = self.node_metrics.get(node)
            connections = metrics.active_connections if metrics else 0
            if connections < min_connections:
                min_connections = connections
                selected_node = node
        
        return selected_node
    
    def _weighted_select(self, nodes: List[str]) -> str:
        """Weighted selection based on node performance."""
        if not self.node_weights:
            return nodes[0]
        
        # Create weighted probability distribution
        weights = [self.node_weights.get(node, 1.0) for node in nodes]
        total_weight = sum(weights)
        
        if total_weight == 0:
            return nodes[0]
        
        # Random weighted selection
        import random
        r = random.uniform(0, total_weight)
        cumulative = 0
        
        for i, weight in enumerate(weights):
            cumulative += weight
            if r <= cumulative:
                return nodes[i]
        
        return nodes[-1]  # Fallback
    
    def _adaptive_select(self, nodes: List[str]) -> str:
        """Adaptive selection combining multiple factors."""
        if not self.node_metrics:
            return self._round_robin_select(nodes)
        
        # Score nodes based on multiple factors
        best_score = -1
        best_node = nodes[0]
        
        for node in nodes:
            metrics = self.node_metrics.get(node)
            if not metrics:
                continue
            
            # Calculate composite score
            response_score = 1.0 / max(metrics.average_response_time, 0.001)
            error_score = 1.0 - metrics.error_rate
            cpu_score = 1.0 - (metrics.cpu_usage / 100.0)
            connection_score = 1.0 / max(metrics.active_connections, 1)
            
            composite_score = (response_score * 0.3 + error_score * 0.3 + 
                             cpu_score * 0.25 + connection_score * 0.15)
            
            if composite_score > best_score:
                best_score = composite_score
                best_node = node
        
        return best_node
    
    def _adapt_strategy(self):
        """Automatically adapt load balancing strategy based on conditions."""
        if not self.node_metrics:
            return
        
        # Analyze current performance patterns
        avg_response_time = sum(m.average_response_time for m in self.node_metrics.values()) / len(self.node_metrics)
        avg_error_rate = sum(m.error_rate for m in self.node_metrics.values()) / len(self.node_metrics)
        
        # Adapt strategy based on conditions
        if avg_error_rate > 0.05:  # High error rate
            self.strategy = LoadBalancingStrategy.LEAST_CONNECTIONS
        elif avg_response_time > 2.0:  # High latency
            self.strategy = LoadBalancingStrategy.WEIGHTED_ROUND_ROBIN
        else:
            self.strategy = LoadBalancingStrategy.ADAPTIVE

class AutoScaler:
    """Intelligent auto-scaling system with predictive capabilities."""
    
    def __init__(self):
        self.scaling_rules: List[Dict] = []
        self.scaling_history: List[Dict] = []
        self.lock = threading.RLock()
        self.min_nodes = 2
        self.max_nodes = 20
        self.scale_cooldown = 300  # 5 minutes
        self.last_scale_time = 0
        
        logger.info("üîß Auto Scaler initialized")
    
    def add_scaling_rule(self, trigger: AutoScalingTrigger, threshold: float, 
                        scale_action: str, cooldown: int = 300):
        """Add auto-scaling rule."""
        rule = {
            "trigger": trigger,
            "threshold": threshold,
            "action": scale_action,  # "scale_up" or "scale_down"
            "cooldown": cooldown
        }
        
        with self.lock:
            self.scaling_rules.append(rule)
        
        logger.info(f"üìà Scaling rule added: {trigger.value} > {threshold} -> {scale_action}")
    
    def evaluate_scaling(self, system_metrics: Dict[str, PerformanceMetrics]) -> Optional[str]:
        """Evaluate if scaling action is needed."""
        if not system_metrics:
            return None
        
        current_time = time.time()
        
        # Check cooldown
        if current_time - self.last_scale_time < self.scale_cooldown:
            return None
        
        with self.lock:
            for rule in self.scaling_rules:
                action = self._evaluate_rule(rule, system_metrics)
                if action:
                    self.last_scale_time = current_time
                    self._record_scaling_event(action, rule, system_metrics)
                    return action
        
        return None
    
    def _evaluate_rule(self, rule: Dict, metrics: Dict[str, PerformanceMetrics]) -> Optional[str]:
        """Evaluate a single scaling rule."""
        trigger = rule["trigger"]
        threshold = rule["threshold"]
        action = rule["action"]
        
        # Aggregate metrics across all nodes
        if trigger == AutoScalingTrigger.REQUEST_RATE:
            total_rps = sum(m.requests_per_second for m in metrics.values())
            if action == "scale_up" and total_rps > threshold:
                return "scale_up"
            elif action == "scale_down" and total_rps < threshold:
                return "scale_down"
        
        elif trigger == AutoScalingTrigger.RESPONSE_TIME:
            avg_response_time = sum(m.average_response_time for m in metrics.values()) / len(metrics)
            if action == "scale_up" and avg_response_time > threshold:
                return "scale_up"
            elif action == "scale_down" and avg_response_time < threshold:
                return "scale_down"
        
        elif trigger == AutoScalingTrigger.CPU_THRESHOLD:
            avg_cpu = sum(m.cpu_usage for m in metrics.values()) / len(metrics)
            if action == "scale_up" and avg_cpu > threshold:
                return "scale_up"
            elif action == "scale_down" and avg_cpu < threshold:
                return "scale_down"
        
        elif trigger == AutoScalingTrigger.QUEUE_LENGTH:
            total_queue_depth = sum(m.queue_depth for m in metrics.values())
            if action == "scale_up" and total_queue_depth > threshold:
                return "scale_up"
            elif action == "scale_down" and total_queue_depth < threshold:
                return "scale_down"
        
        return None
    
    def _record_scaling_event(self, action: str, rule: Dict, metrics: Dict):
        """Record scaling event for analysis."""
        event = {
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "trigger": rule["trigger"].value,
            "threshold": rule["threshold"],
            "metrics_snapshot": {k: v.to_dict() for k, v in metrics.items()}
        }
        
        self.scaling_history.append(event)
        
        # Keep only recent history
        if len(self.scaling_history) > 100:
            self.scaling_history = self.scaling_history[-50:]
        
        logger.info(f"üéØ Auto-scaling event: {action} triggered by {rule['trigger'].value}")

class OptimizedFederatedRouter:
    """
    Generation 3: Optimized and scalable federated router.
    Performance optimization, caching, concurrent processing, auto-scaling.
    """
    
    def __init__(self, privacy_config=None):
        # Initialize from Generation 2 components
        from enhanced_generation_2_robust import RobustPrivacyConfig, SecurityManager, AdvancedHealthMonitor
        
        self.privacy_config = privacy_config or RobustPrivacyConfig()
        self.nodes: Dict[str, Any] = {}
        self.user_budgets: Dict[str, Dict] = {}
        self.request_history: List[Dict] = []
        
        # Generation 3 optimizations
        self.response_cache = AdaptiveCache(max_size=5000, strategy=CacheStrategy.LRU)
        self.metadata_cache = AdaptiveCache(max_size=1000, strategy=CacheStrategy.TTL)
        self.load_balancer = AdaptiveLoadBalancer(LoadBalancingStrategy.ADAPTIVE)
        self.auto_scaler = AutoScaler()
        self.connection_pool = None  # Initialize async
        
        # Performance monitoring
        self.performance_metrics: Dict[str, PerformanceMetrics] = {}
        self.request_executor = ThreadPoolExecutor(max_workers=50, thread_name_prefix="req_processor")
        self.metrics_lock = threading.RLock()
        self.response_times: deque = deque(maxlen=1000)  # Rolling window
        
        # Security and health from Generation 2
        self.security_manager = SecurityManager()
        self.health_monitor = AdvancedHealthMonitor()
        self.rate_limits: Dict[str, List[datetime]] = {}
        
        # Setup auto-scaling rules
        self._setup_auto_scaling_rules()
        
        logger.info("üöÄ Generation 3: Optimized Federated Router initialized")
        logger.info(f"üîß Optimizations: Adaptive Caching, Load Balancing, Auto-Scaling")
    
    def _setup_auto_scaling_rules(self):
        """Setup intelligent auto-scaling rules."""
        self.auto_scaler.add_scaling_rule(
            AutoScalingTrigger.REQUEST_RATE, 100.0, "scale_up", cooldown=180
        )
        self.auto_scaler.add_scaling_rule(
            AutoScalingTrigger.REQUEST_RATE, 20.0, "scale_down", cooldown=300
        )
        self.auto_scaler.add_scaling_rule(
            AutoScalingTrigger.RESPONSE_TIME, 2.0, "scale_up", cooldown=120
        )
        self.auto_scaler.add_scaling_rule(
            AutoScalingTrigger.CPU_THRESHOLD, 80.0, "scale_up", cooldown=180
        )
        self.auto_scaler.add_scaling_rule(
            AutoScalingTrigger.QUEUE_LENGTH, 50, "scale_up", cooldown=120
        )
    
    async def initialize_async_components(self):
        """Initialize async components like connection pool."""
        async def create_mock_connection():
            # Mock connection for demo
            return f"conn_{uuid.uuid4().hex[:8]}"
        
        self.connection_pool = ConnectionPool(create_mock_connection, min_size=10, max_size=100)
        await self.connection_pool.initialize()
        
        logger.info("üîå Async components initialized")
    
    def register_optimized_node(self, node_data: Dict) -> bool:
        """Register node with performance optimization."""
        try:
            node_id = node_data["node_id"]
            
            # Initialize performance metrics
            self.performance_metrics[node_id] = PerformanceMetrics()
            
            # Cache node metadata
            self.metadata_cache.put(f"node:{node_id}", node_data, ttl_seconds=300)
            
            self.nodes[node_id] = node_data
            self.load_balancer.update_node_metrics(node_id, self.performance_metrics[node_id])
            
            logger.info(f"‚úÖ Optimized node registered: {node_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Optimized node registration failed: {e}")
            return False
    
    async def process_optimized_request(self, request_data: Dict, auth_token: str = None) -> Dict:
        """Process request with full Generation 3 optimizations."""
        start_time = time.time()
        request_id = request_data.get("request_id", str(uuid.uuid4()))
        
        try:
            # Check cache first for identical requests
            cache_key = self._generate_cache_key(request_data)
            cached_response = self.response_cache.get(cache_key)
            
            if cached_response:
                logger.info(f"üóÑÔ∏è Cache hit for request {request_id[:8]}...")
                self._update_cache_metrics(True)
                return {
                    **cached_response,
                    "cached": True,
                    "processing_time": time.time() - start_time
                }
            
            self._update_cache_metrics(False)
            
            # Concurrent processing with connection pooling
            conn = await self.connection_pool.acquire()
            
            try:
                # Process with load balancing
                result = await self._process_with_load_balancing(request_data, auth_token, conn)
                
                # Cache successful responses
                if result.get("success"):
                    self.response_cache.put(cache_key, result, ttl_seconds=60)
                
                # Update performance metrics
                processing_time = time.time() - start_time
                self._update_performance_metrics(request_data.get("user_id"), processing_time, result.get("success", False))
                
                # Check auto-scaling triggers
                scaling_action = self.auto_scaler.evaluate_scaling(self.performance_metrics)
                if scaling_action:
                    await self._handle_auto_scaling(scaling_action)
                
                result["processing_time"] = processing_time
                result["optimizations_applied"] = ["caching", "load_balancing", "connection_pooling"]
                
                return result
                
            finally:
                await self.connection_pool.release(conn)
        
        except Exception as e:
            logger.error(f"‚ùå Optimized request processing failed: {e}\n{traceback.format_exc()}")
            processing_time = time.time() - start_time
            self._update_performance_metrics(request_data.get("user_id"), processing_time, False)
            
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
                "request_id": request_id
            }
    
    async def _process_with_load_balancing(self, request_data: Dict, auth_token: str, conn) -> Dict:
        """Process request with intelligent load balancing."""
        # Get available nodes
        available_nodes = [node_id for node_id, node in self.nodes.items() 
                          if node.get("status") == "active"]
        
        if not available_nodes:
            return {
                "success": False,
                "error": "No active nodes available",
                "error_type": "node_failure"
            }
        
        # Select optimal node using load balancer
        selected_node = self.load_balancer.select_node(available_nodes)
        
        if not selected_node:
            return {
                "success": False,
                "error": "Load balancer failed to select node",
                "error_type": "load_balancing_failure"
            }
        
        # Simulate processing with selected node
        await asyncio.sleep(0.05 + (self.performance_metrics[selected_node].active_connections * 0.01))
        
        # Update node metrics
        metrics = self.performance_metrics[selected_node]
        metrics.active_connections += 1
        metrics.requests_per_second += 0.1
        
        # Simulate node processing
        import random
        if random.random() < 0.02:  # 2% simulated failure rate
            metrics.error_rate += 0.01
            return {
                "success": False,
                "error": f"Simulated processing failure on {selected_node}",
                "error_type": "processing_failure",
                "node_id": selected_node
            }
        
        # Success path
        response_text = f"Optimized processing: {request_data.get('prompt', '')[:50]}... (via {selected_node})"
        
        return {
            "success": True,
            "response": response_text,
            "node_id": selected_node,
            "connection_id": conn,
            "load_balancing_strategy": self.load_balancer.strategy.value
        }
    
    def _generate_cache_key(self, request_data: Dict) -> str:
        """Generate cache key for request."""
        # Create key from prompt and key parameters
        key_data = {
            "prompt": request_data.get("prompt", ""),
            "privacy_budget": request_data.get("privacy_budget", 0.1),
            "security_level": request_data.get("security_level", "medium")
        }
        
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _update_cache_metrics(self, cache_hit: bool):
        """Update cache performance metrics."""
        # This would typically update cache hit rate metrics
        pass
    
    def _update_performance_metrics(self, user_id: str, processing_time: float, success: bool):
        """Update comprehensive performance metrics."""
        with self.metrics_lock:
            self.response_times.append(processing_time)
            
            # Update health monitor
            self.health_monitor.record_request(processing_time, success)
            
            # Calculate rolling metrics
            if self.response_times:
                sorted_times = sorted(self.response_times)
                p95_index = int(len(sorted_times) * 0.95)
                p99_index = int(len(sorted_times) * 0.99)
                
                avg_response_time = sum(self.response_times) / len(self.response_times)
                p95_response_time = sorted_times[min(p95_index, len(sorted_times) - 1)]
                p99_response_time = sorted_times[min(p99_index, len(sorted_times) - 1)]
                
                # Update metrics for all nodes (simplified)
                for node_id in self.nodes:
                    if node_id in self.performance_metrics:
                        metrics = self.performance_metrics[node_id]
                        metrics.average_response_time = avg_response_time
                        metrics.p95_response_time = p95_response_time
                        metrics.p99_response_time = p99_response_time
                        metrics.cache_hit_rate = self.response_cache.get_hit_rate()
                        
                        # Simulate other metrics
                        import random
                        metrics.cpu_usage = random.uniform(30, 90)
                        metrics.memory_usage = random.uniform(40, 85)
                        
                        # Update load balancer
                        self.load_balancer.update_node_metrics(node_id, metrics)
    
    async def _handle_auto_scaling(self, action: str):
        """Handle auto-scaling action."""
        logger.info(f"üéØ Executing auto-scaling action: {action}")
        
        if action == "scale_up":
            # Simulate adding new nodes
            new_node_id = f"auto_node_{uuid.uuid4().hex[:8]}"
            new_node = {
                "node_id": new_node_id,
                "endpoint": f"https://{new_node_id}.auto.local:8443",
                "status": "active",
                "auto_created": True
            }
            
            success = self.register_optimized_node(new_node)
            if success:
                logger.info(f"üÜï Auto-scaled up: Added node {new_node_id}")
        
        elif action == "scale_down":
            # Find auto-created nodes to remove
            auto_nodes = [node_id for node_id, node in self.nodes.items() 
                         if node.get("auto_created")]
            
            if auto_nodes:
                remove_node = auto_nodes[0]
                self.nodes.pop(remove_node, None)
                self.performance_metrics.pop(remove_node, None)
                logger.info(f"üóëÔ∏è Auto-scaled down: Removed node {remove_node}")
    
    def get_optimization_status(self) -> Dict:
        """Get comprehensive optimization status."""
        return {
            "caching": {
                "response_cache_size": len(self.response_cache.cache),
                "hit_rate": self.response_cache.get_hit_rate(),
                "strategy": self.response_cache.strategy.value
            },
            "load_balancing": {
                "strategy": self.load_balancer.strategy.value,
                "nodes": len(self.nodes),
                "active_nodes": len([n for n in self.nodes.values() if n.get("status") == "active"])
            },
            "connection_pooling": {
                "active_connections": len(self.connection_pool.active_connections) if self.connection_pool else 0,
                "pool_size": self.connection_pool.pool.qsize() if self.connection_pool else 0
            },
            "auto_scaling": {
                "rules": len(self.auto_scaler.scaling_rules),
                "events": len(self.auto_scaler.scaling_history)
            },
            "performance": {
                "avg_response_time": sum(self.response_times) / len(self.response_times) if self.response_times else 0,
                "request_queue_size": len(self.response_times),
                "total_requests": len(self.request_history)
            }
        }

async def demo_generation_3():
    """Demonstrate Generation 3 optimized functionality."""
    print("üåü GENERATION 3 DEMO: Optimized & Scalable Implementation")
    print("=" * 75)
    
    # Initialize optimized router
    router = OptimizedFederatedRouter()
    await router.initialize_async_components()
    
    # Register optimized nodes
    nodes = [
        {"node_id": "hospital_a", "endpoint": "https://hospital-a.local:8443", "status": "active"},
        {"node_id": "hospital_b", "endpoint": "https://hospital-b.local:8443", "status": "active"},
        {"node_id": "hospital_c", "endpoint": "https://hospital-c.local:8443", "status": "active"},
        {"node_id": "hospital_d", "endpoint": "https://hospital-d.local:8443", "status": "active"}
    ]
    
    for node in nodes:
        router.register_optimized_node(node)
    
    # Generate authentication tokens
    doctor1_token = router.security_manager.generate_token("doctor_001", "cardiology")
    doctor2_token = router.security_manager.generate_token("doctor_002", "neurology")
    
    # Process optimized requests
    requests = [
        {"request_id": str(uuid.uuid4()), "user_id": "doctor_001", 
         "prompt": "Analyze cardiac rhythm patterns for patient with arrhythmia", 
         "privacy_budget": 0.15},
        {"request_id": str(uuid.uuid4()), "user_id": "doctor_002", 
         "prompt": "Review neurological scan for potential brain lesions", 
         "privacy_budget": 0.20},
        {"request_id": str(uuid.uuid4()), "user_id": "doctor_001", 
         "prompt": "Analyze cardiac rhythm patterns for patient with arrhythmia",  # Duplicate for cache demo
         "privacy_budget": 0.15},
        {"request_id": str(uuid.uuid4()), "user_id": "doctor_002", 
         "prompt": "Generate comprehensive patient discharge summary", 
         "privacy_budget": 0.10}
    ]
    
    print("\nüìã Processing Optimized Requests:")
    tokens = {"doctor_001": doctor1_token, "doctor_002": doctor2_token}
    
    # Process requests concurrently to demonstrate performance
    tasks = []
    for request in requests:
        auth_token = tokens.get(request["user_id"])
        task = router.process_optimized_request(request, auth_token)
        tasks.append((request, task))
    
    # Execute concurrently and show results
    for i, (request, task) in enumerate(tasks, 1):
        result = await task
        
        print(f"\n{i}. Request {request['request_id'][:8]}... from {request['user_id']}")
        
        if result["success"]:
            print(f"   ‚úÖ Success: {result['response'][:60]}...")
            print(f"   üè¶ Node: {result.get('node_id')}")
            print(f"   ‚è±Ô∏è Processing time: {result['processing_time']:.3f}s")
            print(f"   üóÑÔ∏è Cached: {result.get('cached', False)}")
            print(f"   üîå Connection: {result.get('connection_id', 'N/A')}")
            print(f"   ‚öñÔ∏è Load balancing: {result.get('load_balancing_strategy', 'N/A')}")
            if result.get("optimizations_applied"):
                print(f"   üöÄ Optimizations: {', '.join(result['optimizations_applied'])}")
        else:
            print(f"   ‚ùå Error: {result['error']}")
            print(f"   ‚è±Ô∏è Processing time: {result['processing_time']:.3f}s")
    
    # Simulate high load to trigger auto-scaling
    print(f"\nüö¶ Simulating High Load (Auto-scaling Demo):")
    high_load_requests = [
        {"user_id": f"doctor_{i:03d}", "prompt": f"High load query {i}", "privacy_budget": 0.05}
        for i in range(20)
    ]
    
    start_time = time.time()
    high_load_tasks = [
        router.process_optimized_request(req, doctor1_token) 
        for req in high_load_requests
    ]
    
    await asyncio.gather(*high_load_tasks)
    high_load_time = time.time() - start_time
    
    print(f"   ‚ö° Processed {len(high_load_requests)} requests in {high_load_time:.2f}s")
    print(f"   üìä Throughput: {len(high_load_requests) / high_load_time:.1f} req/s")
    
    # Show optimization status
    print(f"\nüìä Optimization Status:")
    status = router.get_optimization_status()
    
    print(f"   üóÑÔ∏è Caching: {status['caching']['hit_rate']:.1%} hit rate, "
          f"{status['caching']['response_cache_size']} cached items")
    print(f"   ‚öñÔ∏è Load Balancing: {status['load_balancing']['strategy']}, "
          f"{status['load_balancing']['active_nodes']}/{status['load_balancing']['nodes']} nodes active")
    print(f"   üîå Connection Pool: {status['connection_pooling']['active_connections']} active, "
          f"{status['connection_pooling']['pool_size']} pooled")
    print(f"   üéØ Auto-scaling: {status['auto_scaling']['rules']} rules, "
          f"{status['auto_scaling']['events']} events")
    print(f"   üìà Performance: {status['performance']['avg_response_time']:.3f}s avg response, "
          f"{status['performance']['total_requests']} total requests")
    
    print(f"\n‚úÖ Generation 3 Complete: High-performance optimized system!")
    return router

if __name__ == "__main__":
    asyncio.run(demo_generation_3())