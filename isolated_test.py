#!/usr/bin/env python3
"""
Isolated Test for Research Framework Core Components

Tests research modules in isolation without triggering dependency imports.
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

def test_comparative_analyzer():
    """Test comparative analyzer in isolation."""
    print("üîç Testing ComparativeAnalyzer...")
    
    try:
        # Import only specific classes to avoid full module loading
        sys.path.insert(0, str(Path(__file__).parent / "federated_dp_llm" / "research"))
        
        from comparative_analyzer import (
            ComparativeAnalyzer, BaselineAlgorithm, NovelAlgorithm, 
            AlgorithmType, PerformanceResult
        )
        
        # Test ComparativeAnalyzer
        analyzer = ComparativeAnalyzer()
        assert hasattr(analyzer, 'algorithms')
        assert hasattr(analyzer, 'results_history')
        print("‚úÖ ComparativeAnalyzer initialized successfully")
        
        # Test BaselineAlgorithm
        baseline = BaselineAlgorithm("test_baseline")
        assert baseline.name == "test_baseline"
        metrics = baseline.get_metrics()
        assert isinstance(metrics, dict)
        assert "accuracy" in metrics
        print("‚úÖ BaselineAlgorithm works correctly")
        
        # Test NovelAlgorithm
        novel = NovelAlgorithm("test_novel", "quantum_inspired")
        assert novel.name == "test_novel"
        novel_metrics = novel.get_metrics()
        assert novel_metrics["accuracy"] > metrics["accuracy"]
        print("‚úÖ NovelAlgorithm shows improvements")
        
        # Test algorithm registration
        analyzer.register_algorithm(baseline, AlgorithmType.BASELINE)
        analyzer.register_algorithm(novel, AlgorithmType.NOVEL)
        assert len(analyzer.algorithms) == 2
        print("‚úÖ Algorithm registration works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ComparativeAnalyzer test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_experiment_framework():
    """Test experiment framework."""
    print("\nüß™ Testing ExperimentFramework...")
    
    try:
        from experiment_framework import (
            ExperimentFramework, ExperimentConfig, ExperimentType,
            StatisticalAnalyzer
        )
        
        # Test ExperimentConfig
        config = ExperimentConfig(
            experiment_id="test_001",
            experiment_type=ExperimentType.COMPARATIVE_STUDY,
            name="Test Experiment",
            description="Testing",
            hypothesis="Test hypothesis",
            success_criteria={"accuracy": 0.8}
        )
        
        assert config.experiment_id == "test_001"
        assert config.num_runs == 10
        print("‚úÖ ExperimentConfig created successfully")
        
        # Test ExperimentFramework
        framework = ExperimentFramework(config)
        assert framework.config == config
        assert isinstance(framework.results, list)
        print("‚úÖ ExperimentFramework initialized")
        
        # Test StatisticalAnalyzer
        control = [0.75, 0.76, 0.74, 0.77, 0.75]
        treatment = [0.85, 0.84, 0.86, 0.85, 0.87]
        
        effect_size = StatisticalAnalyzer.calculate_effect_size(control, treatment)
        assert "cohens_d" in effect_size
        assert effect_size["cohens_d"] > 0
        print("‚úÖ Statistical analysis works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ExperimentFramework test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_benchmarking_suite():
    """Test benchmarking suite."""
    print("\nüìä Testing BenchmarkingSuite...")
    
    try:
        from benchmarking_suite import (
            BenchmarkingSuite, DatasetManager, BenchmarkType, 
            PerformanceMetrics
        )
        
        # Test PerformanceMetrics
        metrics = PerformanceMetrics(
            accuracy=0.85, precision=0.83, recall=0.87, f1_score=0.85,
            latency_p50_ms=100, latency_p95_ms=200, latency_p99_ms=300,
            throughput_rps=150, memory_usage_mb=512, cpu_utilization_percent=70,
            gpu_utilization_percent=45, network_io_mbps=100, disk_io_mbps=50,
            energy_consumption_j=2.0, privacy_epsilon_consumed=0.1,
            error_rate=0.02, availability_percent=99.5
        )
        
        composite_score = metrics.calculate_composite_score()
        assert 0.0 <= composite_score <= 1.0
        print("‚úÖ PerformanceMetrics works correctly")
        
        # Test DatasetManager
        import tempfile
        with tempfile.TemporaryDirectory() as temp_dir:
            dataset_manager = DatasetManager(temp_dir)
            assert hasattr(dataset_manager, 'datasets')
            assert hasattr(dataset_manager, 'cache_dir')
            print("‚úÖ DatasetManager initialized")
        
        # Test BenchmarkingSuite
        suite = BenchmarkingSuite()
        assert hasattr(suite, 'dataset_manager')
        assert hasattr(suite, 'benchmarks')
        print("‚úÖ BenchmarkingSuite initialized")
        
        return True
        
    except Exception as e:
        print(f"‚ùå BenchmarkingSuite test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_publication_tools():
    """Test publication tools."""
    print("\nüìù Testing Publication Tools...")
    
    try:
        from publication_tools import (
            PublicationGenerator, PublicationConfig, PublicationType,
            VisualizationDashboard, AcademicFormatter
        )
        
        # Test PublicationConfig
        config = PublicationConfig(
            title="Test Publication",
            authors=["Test Author"],
            institution="Test Institution",
            abstract="Test abstract",
            keywords=["test", "publication"],
            publication_type=PublicationType.TECHNICAL_REPORT
        )
        
        assert config.title == "Test Publication"
        assert len(config.authors) == 1
        print("‚úÖ PublicationConfig created")
        
        # Test AcademicFormatter
        statistical_results = {
            "descriptive_statistics": {
                "baseline": {
                    "accuracy_mean": 0.75,
                    "accuracy_std": 0.02,
                    "sample_size": 10
                }
            }
        }
        
        formatted = AcademicFormatter.format_statistical_results(statistical_results)
        assert "## Results" in formatted
        assert "baseline" in formatted
        print("‚úÖ AcademicFormatter works")
        
        # Test VisualizationDashboard
        import tempfile
        with tempfile.TemporaryDirectory() as temp_dir:
            dashboard = VisualizationDashboard(temp_dir)
            assert hasattr(dashboard, 'output_dir')
            print("‚úÖ VisualizationDashboard initialized")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Publication tools test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_adaptive_optimizer():
    """Test adaptive optimizer."""
    print("\nüîÑ Testing AdaptiveOptimizer...")
    
    try:
        # Add adaptive path
        sys.path.insert(0, str(Path(__file__).parent / "federated_dp_llm" / "adaptive"))
        
        from adaptive_optimizer import (
            AdaptiveOptimizer, OptimizationStrategy, LearningMode
        )
        
        # Test AdaptiveOptimizer
        optimizer = AdaptiveOptimizer(
            optimization_strategy=OptimizationStrategy.GREEDY,
            learning_mode=LearningMode.ONLINE
        )
        
        assert optimizer.optimization_strategy == OptimizationStrategy.GREEDY
        assert hasattr(optimizer, 'current_configuration')
        print("‚úÖ AdaptiveOptimizer initialized")
        
        # Test performance recording
        triggered = await optimizer.record_performance(
            accuracy=0.85,
            latency_p95=120.0,
            throughput=150.0,
            resource_utilization=0.7,
            privacy_budget_remaining=0.5,
            error_rate=0.02
        )
        
        assert len(optimizer.performance_history) > 0
        print("‚úÖ Performance recording works")
        
        # Test configuration validation
        valid_config = {
            "privacy_noise_multiplier": 1.1,
            "batch_size": 32,
            "timeout_ms": 5000
        }
        
        assert optimizer._validate_configuration(valid_config)
        print("‚úÖ Configuration validation works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå AdaptiveOptimizer test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_self_healing_system():
    """Test self-healing system."""
    print("\nüõ°Ô∏è Testing SelfHealingSystem...")
    
    try:
        from self_healing_system import (
            SelfHealingSystem, HealthStatus, RecoveryActionType
        )
        
        # Test SelfHealingSystem
        healer = SelfHealingSystem(
            health_check_interval=1.0,
            critical_threshold=0.3
        )
        
        assert healer.system_status == HealthStatus.HEALTHY
        assert len(healer.available_actions) > 0
        print("‚úÖ SelfHealingSystem initialized")
        
        # Test recovery action types
        action_types = {action.action_type for action in healer.available_actions}
        assert RecoveryActionType.RESTART_COMPONENT in action_types
        assert RecoveryActionType.SCALE_UP_RESOURCES in action_types
        print("‚úÖ Recovery actions available")
        
        # Test health summary
        summary = healer.get_health_summary()
        assert "current_status" in summary
        assert summary["current_status"] == "healthy"
        print("‚úÖ Health summary generation works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå SelfHealingSystem test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_numpy_fallback():
    """Test numpy fallback system."""
    print("\nüî¢ Testing NumPy fallback system...")
    
    try:
        # Add quantum_planning path
        sys.path.insert(0, str(Path(__file__).parent / "federated_dp_llm" / "quantum_planning"))
        
        from numpy_fallback import get_numpy_backend, quantum_wavefunction
        
        has_numpy, np = get_numpy_backend()
        
        if has_numpy:
            print("‚úÖ NumPy is available")
            # Test basic operation
            arr = np.array([1, 2, 3, 4, 5])
            mean_val = np.mean(arr)
            assert mean_val == 3.0
            print("‚úÖ NumPy operations work")
        else:
            print("‚ö†Ô∏è  NumPy not available, using fallback")
            # Test fallback doesn't crash
            assert np is not None
            print("‚úÖ Fallback system operational")
        
        # Test quantum functions
        wavefunction = quantum_wavefunction([0.6, 0.8])
        assert isinstance(wavefunction, (list, tuple)) or (hasattr(wavefunction, '__iter__'))
        print("‚úÖ Quantum functions work with fallback")
        
        return True
        
    except Exception as e:
        print(f"‚ùå NumPy fallback test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def run_isolated_tests():
    """Run all isolated tests."""
    print("üöÄ Starting Isolated Research Framework Tests")
    print("=" * 60)
    
    tests = [
        ("NumPy Fallback", test_numpy_fallback, False),
        ("ComparativeAnalyzer", test_comparative_analyzer, False),
        ("ExperimentFramework", test_experiment_framework, False),
        ("BenchmarkingSuite", test_benchmarking_suite, False),
        ("Publication Tools", test_publication_tools, False),
        ("AdaptiveOptimizer", test_adaptive_optimizer, True),  # Async
        ("SelfHealingSystem", test_self_healing_system, False),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func, is_async in tests:
        print(f"\nüìã Running {test_name} test...")
        try:
            if is_async:
                result = await test_func()
            else:
                result = test_func()
                
            if result:
                passed += 1
                print(f"‚úÖ {test_name} - PASSED")
            else:
                print(f"‚ùå {test_name} - FAILED")
        except Exception as e:
            print(f"üí• {test_name} - ERROR: {e}")
    
    # Print summary
    print("\n" + "=" * 60)
    print("üìä TEST SUMMARY")
    print("=" * 60)
    print(f"Total tests: {total}")
    print(f"Passed: {passed}")
    print(f"Failed: {total - passed}")
    print(f"Success rate: {(passed / total * 100):.1f}%")
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED! Research framework core is working correctly.")
        return True
    elif passed >= total * 0.8:
        print(f"\n‚úÖ Most tests passed ({passed}/{total}). Research framework is functional.")
        return True
    else:
        print(f"\n‚ö†Ô∏è  Several tests failed. Some functionality may not work.")
        return False

if __name__ == "__main__":
    success = asyncio.run(run_isolated_tests())
    sys.exit(0 if success else 1)